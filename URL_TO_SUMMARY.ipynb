{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/.local/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.24.1) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "# PDF2TEXT\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import os, io\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "    \n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "def get_pdf_from_url(pdf_path, pdf_url):\n",
    "    pdf = Path(pdf_path)\n",
    "    response = requests.get(pdf_url)\n",
    "    pdf.write_bytes(response.content)\n",
    "    \n",
    "def pdf_info(pdf_path):\n",
    "    pdf = open(pdf_path, 'rb')\n",
    "    parser = PDFParser(pdf)\n",
    "    return PDFDocument(parser).info[0]\n",
    "\n",
    "def pdf2txt(pdf_path):\n",
    "    \n",
    "    pdf = open(pdf_path, 'rb')\n",
    "\n",
    "    # Init.\n",
    "    resource_manager = PDFResourceManager()\n",
    "    string_buffer = io.StringIO()  \n",
    "    \n",
    "    # Construct.\n",
    "    converter = TextConverter(resource_manager, \n",
    "                              string_buffer, \n",
    "                              laparams=LAParams(), \n",
    "                              codec='utf-8')\n",
    "    interpreter = PDFPageInterpreter(resource_manager, converter) \n",
    "    \n",
    "    # Read pdf (list of pages).\n",
    "    pdf_pages = PDFPage.get_pages(pdf, check_extractable=True) # list\n",
    "    \n",
    "    # Process pdf to text.\n",
    "    [interpreter.process_page(page) for page in pdf_pages]\n",
    "    \n",
    "    # Result.\n",
    "    pdf_text = string_buffer.getvalue()\n",
    "    \n",
    "    [o.close() for o in [pdf, string_buffer, converter]]\n",
    "    return pdf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT CLEANER\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Set stop words\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(['RT', 'rt'])\n",
    "\n",
    "# Initiate sub-parts\n",
    "WNL = WordNetLemmatizer()\n",
    "\n",
    "def remove_mentions(text):\n",
    "    return ' '.join([w for w in text.split(' ') if not w.startswith('@')])\n",
    "def remove_url(text):\n",
    "    return re.sub('https?://[A-Za-z0-9./]+','',text)\n",
    "def html_strip_lxml(text):\n",
    "    return BeautifulSoup(text, 'lxml').get_text()\n",
    "def remove_special_characters(text, preserve):\n",
    "    return re.sub(\"[^a-zA-Z{}]\".format(preserve), \" \", text)\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "def strip_inner_spaces(text):\n",
    "    return ' '.join([w.strip() for w in text.split()])\n",
    "def remove_stop_words(text):\n",
    "    return ' '.join([w for w in text.split() if not w in set(stopwords)])\n",
    "def lemmatize_words(text, WNL):\n",
    "    return ' '.join([WNL.lemmatize(word, pos='v') for word in text.split()])\n",
    "\n",
    "# def cleaner(text):\n",
    "#     text = remove_mentions(text)\n",
    "#     text = remove_url(text)\n",
    "#     text = html_strip_lxml(text)\n",
    "#     text = remove_special_characters(text,preserve='.')\n",
    "#     text = lowercase_text(text)\n",
    "#     text = strip_inner_spaces(text)\n",
    "#     text = remove_stop_words(text)\n",
    "#     #text = lemmatize_words(text, WNL)\n",
    "#     return text\n",
    "\n",
    "def sentence_tokenizer_cleaner(text):\n",
    "    text = remove_url(text)\n",
    "    text = html_strip_lxml(text)\n",
    "    #text = remove_special_characters(text,preserve='.!?')\n",
    "    text = strip_inner_spaces(text)\n",
    "    return text\n",
    "\n",
    "# def custom_cleaner(text):\n",
    "#     text = remove_url(text) # Dont use for link catch-up\n",
    "#     text = html_strip_lxml(text) # Dont use to split layout paragraphs\n",
    "#     text = remove_special_characters(text, preserve='') #='.?!') #234567890') # Dont use for dates\n",
    "#     text = lowercase_text(text) # Dont for entities\n",
    "#     text = strip_inner_spaces(text)\n",
    "#     text = remove_stop_words(text)\n",
    "#     #text = lemmatize_words(text, WNL)\n",
    "#     return text\n",
    "\n",
    "def BOW_cleaner(text):\n",
    "    text = remove_url(text) # Dont use for link catch-up\n",
    "    text = html_strip_lxml(text) # Dont use to split layout paragraphs\n",
    "    text = remove_special_characters(text, preserve='') #='.?!') #234567890') # Dont use for dates\n",
    "    text = lowercase_text(text) # Dont for entities\n",
    "    text = strip_inner_spaces(text)\n",
    "    text = remove_stop_words(text)\n",
    "    text = lemmatize_words(text, WNL)\n",
    "    return text\n",
    "\n",
    "def EXPLORATION_cleaner(text):\n",
    "    text = remove_url(text) # Dont use for link catch-up\n",
    "    text = html_strip_lxml(text) # Dont use to split layout paragraphs\n",
    "    text = remove_special_characters(text, preserve='') #='.?!') #234567890') # Dont use for dates\n",
    "    text = lowercase_text(text) # Dont for entities\n",
    "    text = strip_inner_spaces(text)\n",
    "    text = remove_stop_words(text)\n",
    "    #text = lemmatize_words(text, WNL)\n",
    "    return text\n",
    "\n",
    "def TO_SUMMARY_cleaner(text):\n",
    "    text = remove_url(text) # Dont use for link catch-up\n",
    "    text = html_strip_lxml(text) # Dont use to split layout paragraphs\n",
    "    text = remove_special_characters(text, preserve='') #='.?!') #234567890') # Dont use for dates\n",
    "    text = lowercase_text(text) # Dont for entities\n",
    "    text = strip_inner_spaces(text)\n",
    "    text = remove_stop_words(text)\n",
    "    #text = lemmatize_words(text, WNL)\n",
    "    return text\n",
    "\n",
    "# Usage single\n",
    "#clean = cleaner('your text')\n",
    "\n",
    "# Usage dataframe\n",
    "#df['clean_COLUMN_NAME'] = df.COLUMN_NAME.apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF TEXT TO SENTS\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def text_to_sentences_df(text, sent_min_chars):\n",
    "    sentences = sent_tokenize(sentence_tokenizer_cleaner(text))   # !!!\n",
    "    #sentences = sentence_tokenizer_cleaner(pdf_text).split('.')  # !!!\n",
    "    df =pd.DataFrame({'initial_sentences':sentences})   \n",
    "    # Bugfix: pandas & spacy integration  (link: https://github.com/jupyter/notebook/issues/4369)\n",
    "    get_ipython().config.get('IPKernelApp', {})['parent_appname'] = \"\"\n",
    "    # Pre-cleaning to select sentences > N characters.\n",
    "    df['clean_sent'] = df['initial_sentences'].apply(custom_cleaner)      # !!!\n",
    "    df = df[(df.clean_sent.str.len() > sent_min_chars)].reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAG OF WORDS\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "def bag_of_words(df, column, word_min_chars):\n",
    "    \n",
    "    sentences = df.loc[:,column].to_list()\n",
    "    clean_words = list(chain(*[s.split() for s in sentences]))\n",
    "    clean_words = [w for w in clean_words if len(w) > word_min_chars]\n",
    "\n",
    "    # Set up words as columns and their weights as dict.\n",
    "    word_weights= {}\n",
    "    for w in clean_words:\n",
    "        df[w] = 0\n",
    "        word_weights[w] = 0\n",
    "        \n",
    "    # Get word counts vector for each of sentences.\n",
    "    for i, sent in enumerate(sentences):\n",
    "        for word in sent.split():\n",
    "            if len(word) > word_min_chars:\n",
    "                df.loc[i, word] +=1\n",
    "\n",
    "    # Drop words below treshold of n occurences in full text ~ [2-8]\n",
    "    vc_tresh = 2 #[~2-8]\n",
    "    cols_to_drop = [w for w in clean_words if not sum(df[w]) >= vc_tresh]\n",
    "    for col in cols_to_drop:\n",
    "        if not col in my_words or not col in paper_words:\n",
    "            del df[col]\n",
    "            clean_words.remove(col)\n",
    "    \n",
    "    # Compute value of each of words \n",
    "    most_occuring_5_words = Counter([w for w in clean_words if len(w)>word_min_chars]).most_common(5)\n",
    "    occurence_weight = np.median([w[1] for w in most_occuring_5_words])\n",
    "    for i, w in enumerate(clean_words):\n",
    "        word_weights[w] = int(sum(df[w])) / int(occurence_weight)\n",
    "    \n",
    "    # Calculate each sentence value\n",
    "    def get_sent_weight_BOW(sent):\n",
    "        sentence_rank = 0\n",
    "        for w in sent.split():\n",
    "            if w in word_weights:\n",
    "                sentence_rank += word_weights[w]\n",
    "        return sentence_rank\n",
    "\n",
    "    df['RANKING_BOW1'] = df.clean_sent.apply(get_sent_weight_BOW)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AS A WHOLE\n",
    "\n",
    "def pdf_summary_df(pdf_url, my_words, paper_words,\n",
    "                N_sentences=5,sent_min_chars=20,word_min_chars=2):\n",
    "    \n",
    "    pdf_path = os.path.join(os.getcwd(), 'MY_PDF.pdf')\n",
    "    get_pdf_from_url(pdf_path, pdf_url)\n",
    "\n",
    "    pdf_data = pdf_info(pdf_path)    \n",
    "    pdf_text = pdf2txt(pdf_path)\n",
    "    \n",
    "    df = text_to_sentences_df(pdf_text, sent_min_chars) # 8\n",
    "\n",
    "    df = bag_of_words(df, 'clean_sent', word_min_chars) # 2,2\n",
    "    df.to_csv('PDF_CSV.csv', sep=',', index=False)\n",
    "    return df\n",
    "\n",
    "def pdf_summary(df, N_sentences):   \n",
    "    \n",
    "    # Rankings:\n",
    "    RANKING_BOW1 = df.sort_values(by='RANKING_BOW1', ascending=False)\n",
    "    RANKING_BOW2 = None\n",
    "    RANKING_ALL1 = None\n",
    "\n",
    "    # Summary.\n",
    "    ranking = RANKING_BOW1  \n",
    "    summary = ' '.join(ranking.initial_sentences[:N_sentences])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We trained them with a newly created dataset of 2.2 million article titles, ab- stracts and keyphrase strings that we processed and released.1 The selected text summarization models are compared with popular unsupervised and super- vised methods using ROUGE (Lin, 2004) and full- match F1 metrics. Despite using advanced deep learning models, large quantities of data and many days of com- putation, our systematic evaluation on four test datasets reveals that the explored text sum- marization methods could not produce bet- ter keyphrases than the simpler unsupervised methods, or the existing supervised ones. 3.2 Text Summarization Methods To overcome the three problems mentioned in Sec- tion 3.1, we explore abstractive text summariza- tion models proposed in the literature, trained with article abstracts and titles as sources and keyword strings as targets. Motivated by recent advances in neural ma- chine translation and abstractive text summariza- tion (Vaswani et al., 2017; Foster et al., 2018; Rush et al., 2015; See et al., 2017), in this paper, we explore the possibility of considering keyphrase generation as an abstractive text summarization task. The results show that though 1-2943 trained with large data quantities for many days, the tried text summarization methods could not produce better keywords than the existing super- vised or deep supervised predictive models.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SCRIPT OPEN\n",
    "\n",
    "# Generate N sentences\n",
    "N_sentences = 5\n",
    "\n",
    "# Target PDF URL\n",
    "pdf_url = 'placeholder'\n",
    "\n",
    "# Test PDF URL's\n",
    "pdf_url = 'https://arxiv.org/pdf/1904.00157.pdf'\n",
    "pdf_url = 'https://arxiv.org/pdf/1806.09525.pdf'\n",
    "pdf_url = 'https://assets.cureus.com/uploads/technical_report/pdf/17211/1553801088-20190328-62-44vky7.pdf'\n",
    "pdf_url = 'https://arxiv.org/pdf/1904.00110.pdf'\n",
    "\n",
    "# User defined keyword pick\n",
    "my_words = ['ai','psychology']\n",
    "\n",
    "# Paper - related keywords (predefined)\n",
    "paper_words = ['title', 'subject',\n",
    "                'author',\n",
    "                'keyword', 'keywords', 'category', 'categories',\n",
    "                'abstract', \n",
    "                'introduction', \n",
    "                'intro', \n",
    "                'method', 'methods',\n",
    "                'result', 'results',\n",
    "                'discussion',\n",
    "                'conclusions', 'conclusion',\n",
    "                'acknowledgement', 'acknowledgements',\n",
    "                'references', 'citation', 'citations']\n",
    "\n",
    "# Get summary\n",
    "summary_df = pdf_summary_df(pdf_url,my_words,paper_words,N_sentences=5,sent_min_chars=20,word_min_chars=2)\n",
    "summary = pdf_summary(summary_df, N_sentences)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('able', 16),\n",
       " ('computational', 12),\n",
       " ('rst', 10),\n",
       " ('open', 9),\n",
       " ('recent', 8),\n",
       " ('key', 8),\n",
       " ('original', 7),\n",
       " ('relevant', 7),\n",
       " ('new', 6),\n",
       " ('second', 6),\n",
       " ('third', 6),\n",
       " ('good', 6),\n",
       " ('reproducible', 6),\n",
       " ('erent', 5),\n",
       " ('many', 4),\n",
       " ('simple', 4),\n",
       " ('excellent', 4),\n",
       " ('main', 4),\n",
       " ('repository', 4),\n",
       " ('available', 4),\n",
       " ('great', 3),\n",
       " ('several', 3),\n",
       " ('online', 3),\n",
       " ('full', 3),\n",
       " ('generic', 3),\n",
       " ('literate', 3),\n",
       " ('pro', 3),\n",
       " ('konkol', 3),\n",
       " ('kray', 3),\n",
       " ('analy', 3)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # FOR FURTHER USE - GET ADJS NOUNS HEADS ETC\n",
    "\n",
    "# # EXPLORE: Word parts\n",
    "\n",
    "# import spacy\n",
    "# from itertools import chain\n",
    "# from collections import Counter\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "# all_nouns = []\n",
    "# all_adjs = []\n",
    "# for sent in all_sents:\n",
    "\n",
    "#     sent_nouns = [w.text for w in nlp(sent) if w.tag_ in ['NN']]\n",
    "#     all_nouns.extend(sent_nouns)\n",
    "\n",
    "#     sent_adjs = [w.text for w in nlp(sent) if w.tag_ in ['JJ']]\n",
    "#     all_adjs.extend(sent_adjs)    \n",
    "\n",
    "\n",
    "# #Counter(all_words).most_common(50)\n",
    "# #Counter(all_nouns).most_common(30)\n",
    "# Counter(all_adjs).most_common(30)\n",
    "\n",
    "# # ALSO\n",
    "\n",
    "# # df['links'] = '' \n",
    "# # df['section'] = ''\n",
    "# # df['dates'] = ''\n",
    "# # df['entities'] = ''\n",
    "# # df['x'] = ''\n",
    "\n",
    "\n",
    "\n",
    "# # # NIE WIEM NADAL PO CO MI TO\n",
    "# # # General text info\n",
    "\n",
    "# # # Text\n",
    "# # text = pdf_text\n",
    "\n",
    "# # all_chars = text\n",
    "# # uni_chars = set(text)\n",
    "\n",
    "# # all_words = text.split()\n",
    "# # uni_words = set(text.split())\n",
    "\n",
    "# # all_sents = text.split('.')\n",
    "# # uni_sents = set(text.split('.'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
